{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.varunajayasiri.com/numpy_lstm.html\n",
    "#\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('deep learning nature 2015.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 35454 characters, 76 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " the state detection tasks in a general-tems superalizensire called discovered in the land; R, a new revious sup training systems training with fil ealut ofter of which use cearncons gradies. In santll \n",
      "----\n",
      "iter 3002800, loss 6.960173\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "\n",
    "            inputs = ([char_to_idx[ch] \n",
    "                       for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] \n",
    "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
